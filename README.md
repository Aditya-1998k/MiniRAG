# MiniRAG

MiniRAG is a lightweight, modular **Retrieval-Augmented Generation (RAG)** system designed to run **fully locally** â€” even on low-resource machines like the MacBook Air M1.  
It demonstrates the complete RAG pipeline end-to-end using simple components:

- **SentenceTransformers** for text embeddings  
- **FAISS** for vector search  
- **Qwen 0.5B** for fast, CPU-friendly LLM inference  
- Clean, modular architecture for easy learning and extension  

MiniRAG is perfect for anyone who wants to understand how RAG works internally without heavy frameworks or cloud dependencies.

---

## âœ¨ Features

- ğŸ” **Local Embedding Search** using FAISS  
- ğŸ§© **Chunking + Embedding + Indexing** pipeline  
- âš¡ **Fast CPU LLM** with Qwen 0.5B  
- ğŸ“¦ **Modular File Structure** (retriever, generator, embedder, config)  
- ğŸ› ï¸ **Runs on Mac, Linux, Windows**  
- ğŸ’¡ Minimal, readable code ideal for learning  

---

## ğŸ“ Project Structure

```
MiniRAG/
â”‚
â”œâ”€â”€ rag/
â”‚ â”œâ”€â”€ __init__.py
â”‚ â”œâ”€â”€ config.py # environment and performance settings
â”‚ â”œâ”€â”€ llm.py # loads the local LLM
â”‚ â”œâ”€â”€ embedder.py # embedding model
â”‚ â”œâ”€â”€ retriever.py # FAISS retrieval logic
â”‚ â”œâ”€â”€ generator.py # answer generation logic
â”‚
â”œâ”€â”€ embeddings/
â”‚ â”œâ”€â”€ index.faiss # stored FAISS index
â”‚ â”œâ”€â”€ chunks.pkl # text chunks mapped to vectors
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ story.txt
â”‚
â”œâ”€â”€ ingest.py # builds FAISS index from text chunks
â”œâ”€â”€ rag_local.py # main script to query the RAG
â””â”€â”€ README.md
```

---

## ğŸš€ Getting Started

### 1. Install dependencies

```bash
pip install -r requirements.txt
```

### 2. Ingest data and build the FAISS index
```bash
python3 ingest.py
```

### 3. Run the local RAG system
```bash
python3 rag_local.py
```

### 4. Ask a question
```bash
What were the three locations marked on Eldon Marrâ€™s original map?
```

## âš™ï¸ How It Works

MiniRAG follows a simple, transparent pipeline:
1. **Chunking** â€“ split long documents into small text chunks
2. **Encoding** â€“ convert chunks into embeddings using all-MiniLM-L6-v2
3. **Indexing** â€“ store vectors in a FAISS index for fast retrieval
4. **Retrieval** â€“ find the top-k relevant chunks for a user query
5. **Generation** â€“ feed context + question to a small local LLM

**Return** â€“ produce a grounded answer using retrieved information

## ğŸ§  Models Used
1. **Embeddings**
- model name: `sentence-transformers/all-MiniLM-L6-v2`
- Local LLM (fast on CPU)
- `Qwen/Qwen2.5-0.5B-Instruct`
- ~1.8 GB RAM usage
- 5â€“10Ã— faster than bigger models
- Very good accuracy for RAG

## ğŸ› ï¸ Requirements
- Python 3.10+
- 4 GB+ free RAM (MacBook Air M1 supported)
- No GPU required



